---
author: joshua-auchincloss
components:
  - compiler
created: 2025-10-31
kind: SPEC
number: 14
status: draft
title: Schema Compilation
updates:
  - author: joshua-auchincloss
    date: 2025-10-31
    description: Created specification
  - author: joshua-auchincloss
    date: 2025-12-24
    description: Updated specification
version_after: 0.1.0
version_before: null
---

# SPEC-0014: Schema Compilation

## Overview

This specification defines the deterministic rules for schema-level compilation in multi-schema workspaces. It prescribes exact algorithms for dependency loading, graph construction, topological ordering, and parallel execution. The specification ensures all compiler implementations compile schemas in the same order with identical results.

## Motivation

Compilers must implement schema compilation consistently to ensure:

- **Deterministic ordering**: Same workspace produces same compilation order across implementations
- **Correctness**: Dependencies compile before dependents
- **Parallelism**: Independent schemas compile concurrently for optimal throughput
- **Error detection**: Circular dependencies detected before compilation begins

This specification provides the normative implementation guide for schema compilation.

## Compilation Pipeline Diagram

![Schema Compilation Flow](../../../../diagrams/schema_compilation_flow.png)

_Figure 1: Complete schema compilation pipeline. Phase 1 loads all dependencies in parallel. Phase 2 constructs dependency graph, detects cycles, and computes topological order. Phase 3 compiles schemas level-wise and registers types. Phase 4 resolves types within each namespace using TypeResolver._

## Rules

### Rule 1: Dependency Loading Algorithm

**Requirement**: The compiler MUST load all transitive dependencies using the following algorithm.

**Implementation** (from `parser/src/ctx/compile/loader.rs`):

```rust
pub async fn load_dependencies_parallel(
    root: &SchemaCtx,
    state: Arc<RwLock<SharedCompilationState>>,
    resolver: Arc<PackageResolver>,
    cache: SchemaCache,
    type_registry: TypeRegistry,
    root_path: PathBuf,
    max_concurrent_tasks: usize,
    progress: &CompilationProgress,
) -> Result<()>
```

**Algorithm**:

1. **Initialize Channels**:
   - Create unbounded task channel `(task_tx, task_rx)`
   - Create unbounded result channel `(result_tx, result_rx)`

2. **Seed Task Queue**:

   ```rust
   for ns_ctx in root.namespaces.values() {
       for import in &ns_ctx.lock().await.imports {
           let pkg = import.value.as_ref_context().package.clone();
           if !root.namespaces.contains_key(&pkg) {
               task_tx.send(CompilationTask {
                   package_name: pkg.to_string(),
                   dependency_chain: vec![root.package.package.name.clone()],
               }).await?;
           }
       }
   }
   ```

3. **Close Task Sender**: `drop(task_tx)` to signal workers when queue is empty

4. **Spawn Workers**:

   ```rust
   for worker_id in 0..max_concurrent_tasks.max(1) {
       let worker = Box::pin(async move {
           while let Ok(task) = task_rx.recv().await {
               match Self::process_dependency_task(task, /* ... */).await {
                   Ok(result) => { result_tx.send(Ok(result)).await?; }
                   Err(e) => {
                       result_tx.send(Err(e)).await?;
                       break;
                   }
               }
           }
       });
       workers.push(tokio::spawn(worker));
   }
   ```

5. **Spawn Result Collector**:

   ```rust
   let result_collector = tokio::spawn(async move {
       while let Ok(result_or_error) = result_rx.recv().await {
           match result_or_error {
               Ok(DependencyTaskResult::Loaded(result)) => {
                   state.write().await.dependencies.insert(
                       result.package_name.clone(),
                       result.schema.clone(),
                   );
                   state.write().await.processing_set.remove(&result.package_name);
               }
               Ok(DependencyTaskResult::AlreadyLoaded) => { /* skip */ }
               Err(e) => { first_error = Some(e); }
           }
       }
       first_error.map(Err).unwrap_or(Ok(()))
   });
   ```

6. **Close Result Sender**: `drop(result_tx)` after all workers spawned

7. **Await Completion**:

   ```rust
   let collector_result = result_collector.await;
   for worker in futures_util::future::join_all(workers).await {
       worker?;
   }
   collector_result??;
   ```

**Error Condition**: If any worker encounters an error, it MUST:

1. Send error to result channel
2. Break worker loop immediately
3. Result collector MUST capture first error and propagate after workers complete

### Rule 2: Circular Dependency Detection

**Requirement**: The compiler MUST detect circular schema dependencies during topological grouping.

Note: The `processing_set` check in the dependency loader is currently deferred (TODO). Circular dependencies are detected during `SchemaDependencyGraph::topological_groups()` instead.

### Rule 3: Dependency Graph Construction

**Requirement**: The compiler MUST build a complete dependency graph from all loaded schemas.

**Graph Structure** (from `parser/src/ctx/graph/schemas.rs`):

```rust
pub struct SchemaDependencyGraph {
    pub(crate) nodes: BTreeMap<CacheKey, SchemaNode>,
}

pub struct SchemaNode {
    pub id: CacheKey,
    pub imports: Vec<Import>,
    pub depends_on: Vec<CacheKey>,
}
```

Note: Uses `BTreeMap` for deterministic iteration order.

**Implementation** (from `parser/src/ctx/compile/schema_compiler.rs:build_graph`):

```rust
async fn build_graph(ctx: &CompileCtx) -> Result<SchemaDependencyGraph> {
    let mut graph = SchemaDependencyGraph::new();

    // Add root schema
    let root_key = Self::build_cache_key_for_schema(&ctx.root)?;
    let root_imports = Self::extract_imports(&ctx.root).await;
    graph.add_schema(root_key.clone(), root_imports);

    // Add all dependency schemas
    let state = ctx.state.read().await;
    for schema in state.dependencies.values() {
        let dep_key = Self::build_cache_key_for_schema(schema)?;
        let dep_imports = Self::extract_imports(schema).await;
        graph.add_schema(dep_key, dep_imports);
    }
    drop(state);

    // Build dependency edges
    graph.build_dependencies();

    Ok(graph)
}
```

**Algorithm**:

1. **Extract Root Imports**: Traverse all namespaces in root schema, collect import statements
2. **Extract Dependency Imports**: For each loaded dependency, collect import statements
3. **Resolve Import Names**: Map import package names to schema IDs via `CacheKey`
4. **Build Edges**: Populate `depends_on` field for each schema node

**Import Extraction**:

```rust
async fn extract_imports(schema: &SchemaCtx) -> Vec<Import> {
    let mut imports = Vec::new();
    for ns_ctx in schema.namespaces.values() {
        let ns = ns_ctx.lock().await;
        for import in &ns.imports {
            imports.push(Import {
                name: import.value.as_ref_context().package.to_string(),
                resolved_id: None,
            });
        }
    }
    imports
}
```

### Rule 4: Schema Cycle Detection

**Requirement**: The compiler MUST detect circular schema dependencies using strongly connected components.

**Implementation** (from `parser/src/ctx/compile/schema_compiler.rs:compile_all`):

```rust
let successors_fn = |node: &CacheKey| -> Vec<CacheKey> { graph.successors(node) };

let components = strongly_connected_components(
    &graph.nodes.keys().cloned().collect::<Vec<_>>(),
    successors_fn,
);

for component in components {
    if component.len() > 1 {
        let cycle_names: Vec<String> = component
            .iter()
            .map(|k| k.package_name.clone())
            .collect();

        return Err(crate::Error::SchemaCircularDependency {
            schemas: cycle_names,
        });
    }
}
```

**Algorithm**:

1. **Compute SCC**: Use Tarjan's algorithm from `pathfinding` crate
2. **Check Component Size**: If any component has size > 1, it's a cycle
3. **Extract Schema Names**: Collect package names from all schemas in cycle
4. **Return Error**: `Error::SchemaCircularDependency { schemas }`

**Error Message Format**:

```text
Circular schema dependency detected: schema_a, schema_b, schema_c
```

### Rule 5: Topological Grouping

**Requirement**: The compiler MUST group schemas into compilation levels using topological sort.

**Implementation** (from `parser/src/ctx/graph/schemas.rs:topological_groups`):

```rust
pub fn topological_groups(&self) -> Result<Vec<Vec<CacheKey>>> {
    let mut level_map: HashMap<CacheKey, usize> = HashMap::new();
    let mut queue: VecDeque<(CacheKey, usize)> = VecDeque::new();

    // Seed with leaf nodes (no dependencies) at level 0
    for (schema_id, node) in &self.nodes {
        if node.depends_on.is_empty() {
            queue.push_back((schema_id.clone(), 0));
            level_map.insert(schema_id.clone(), 0);
        }
    }

    // BFS to propagate levels to successors
    while let Some((current, level)) = queue.pop_front() {
        for successor in self.successors(&current) {
            let successor_level = level + 1;
            if successor_level > level_map.get(&successor).copied().unwrap_or(0) {
                level_map.insert(successor.clone(), successor_level);
                queue.push_back((successor, successor_level));
            }
        }
    }

    // Detect missing (unreachable) nodes -> indicates cycle
    if level_map.len() != self.nodes.len() {
        let missing: Vec<String> = self.nodes.keys()
            .filter(|id| !level_map.contains_key(id))
            .map(|id| id.package_name.clone())
            .collect();
        return Err(Error::SchemaCircularDependency { schemas: missing });
    }

    // Group by level
    let max_level = level_map.values().max().copied().unwrap_or(0);
    let mut groups = vec![Vec::new(); max_level + 1];
    for (schema_id, level) in level_map {
        groups[level].push(schema_id);
    }

    Ok(groups)
}
```

**Algorithm**:

1. **Identify Leaves**: Schemas with `depends_on.is_empty()` start at level 0
2. **BFS Propagation**: Each successor has level = max(current level of that successor, parent level + 1)
3. **Cycle Detection**: If any node unreachable, circular dependency exists
4. **Result**: `Vec<Vec<CacheKey>>` where `groups[i]` contains schemas at level i

**Successor Semantics**:

```rust
// If B depends on A, then A's successor is B
// This ensures A is processed before B in topological order
pub fn successors(&self, schema_id: &CacheKey) -> Vec<CacheKey> {
    self.nodes
        .iter()
        .filter_map(|(id, node)| {
            if node.depends_on.contains(schema_id) {
                Some(id.clone())
            } else {
                None
            }
        })
        .collect()
}
```

**Guarantee**: For all schemas S in `groups[N]`:

- S depends only on schemas in `groups[0..N]`
- All schemas in `groups[N]` are independent (no dependencies among them)

### Rule 6: Schema Compilation

**Requirement**: The compiler MUST compile all schemas in topological level order.

**Implementation** (from `parser/src/ctx/compile/schema_compiler.rs:compile_all`):

```rust
for (level, group) in groups.iter().enumerate() {
    for schema_id in group {
        compile_bar.set_message(format!("compiling {}", schema_id.package_name));
        Self::compile_schema(ctx, schema_id).await?;
        compile_bar.set_message(format!("completed {}", schema_id.package_name));
        compile_bar.inc(1);
    }
}
```

**Algorithm**:

1. **Sequential Levels**: Groups MUST execute sequentially (level N+1 after level N completes)
2. **Sequential Within Level**: Schemas within a level currently compile sequentially (not parallel via `try_join_all`)
3. **Error Propagation**: If any schema fails, immediately return error
4. **Progress Update**: Increment progress bar after each schema completes

Note: Within-level parallelism is not currently implemented.

### Rule 7: Namespace Ordering

**Requirement**: Within each schema, the compiler MUST order namespaces by depth using BFS.

**Implementation** (from `parser/src/ctx/compile/schema_compiler.rs:namespace_levels`):

```rust
async fn namespace_levels(schema: &SchemaCtx) -> Vec<Vec<String>> {
    let mut levels: Vec<Vec<String>> = Vec::new();
    let mut visited = HashSet::new();
    let mut queue = VecDeque::new();

    // Find root namespaces (no parent)
    for (ns_name, ns_ctx) in &schema.namespaces {
        let ns = ns_ctx.lock().await;
        if ns.parent.is_none() {
            queue.push_back((ns_name.clone(), 0));
        }
    }

    // BFS traversal
    while let Some((ns_name, depth)) = queue.pop_front() {
        if visited.contains(&ns_name) {
            continue;
        }
        visited.insert(ns_name.clone());

        // Add to appropriate level
        while levels.len() <= depth {
            levels.push(Vec::new());
        }
        levels[depth].push(ns_name.clone());

        // Enqueue children
        if let Some(ns_ctx) = schema.namespaces.get(&ns_name) {
            let ns = ns_ctx.lock().await;
            for child_name in &ns.children_names {
                queue.push_back((child_name.clone(), depth + 1));
            }
        }
    }

    levels
}
```

**Algorithm**:

1. **Identify Roots**: Namespaces with `parent.is_none()` are roots (depth 0)
2. **BFS Traversal**: Process namespaces level by level
3. **Depth Assignment**: Namespace at depth D has parent at depth D-1
4. **Result**: `Vec<Vec<String>>` where `levels[D]` contains all depth-D namespaces

**Guarantee**: Namespace N at depth D only references types from namespaces at depth 0..D.

### Rule 8: Type Registration

**Requirement**: The compiler MUST register all types in a namespace before compiling deeper namespaces.

**Implementation** (from `parser/src/ctx/compile/schema_compiler.rs:compile_schema`):

```rust
for (depth, group) in levels.into_iter().enumerate() {
    let tasks: Vec<_> = group
        .iter()
        .map(|ns_name| {
            let schema = Arc::clone(&schema);
            let ns_name = ns_name.clone();
            async move {
                Self::register_types_recursive(&schema, &ns_name, depth).await
            }
        })
        .collect();

    futures_util::future::try_join_all(tasks).await?;
}
```

**Type Registration Algorithm**:

```rust
async fn register_types_recursive(
    schema: &Arc<SchemaCtx>,
    ns_name: &str,
    depth: usize,
) -> Result<()> {
    let ns = schema.get_namespace(ns_name)?;
    let ns_guard = ns.lock().await;

    for (item_ctx, child) in &ns_guard.children {
        match &child.value {
            NamespaceChild::Struct(struct_def) => {
                // Register in TypeRegistry
                schema.type_registry.register_struct(
                    &ns_name,
                    struct_def,
                )?;
            }
            NamespaceChild::Enum(enum_def) => {
                schema.type_registry.register_enum(
                    &ns_name,
                    enum_def,
                )?;
            }
            _ => { /* skip non-types */ }
        }
    }

    Ok(())
}
```

**Guarantee**: When namespace at depth D+1 compiles, all types from depth 0..D are registered.

### Rule 9: Parallel Type Resolution

**Requirement**: The compiler MUST resolve types for all schemas in a topological group concurrently.

**Implementation** (from `parser/src/ctx/compile/schema_compiler.rs:compile_all`):

```rust
for (level, group) in groups.iter().enumerate() {
    let resolution_tasks: Vec<_> = group
        .iter()
        .map(|schema_id| Self::resolve_schema_types(ctx, schema_id, &resolution_bar))
        .collect();

    futures_util::future::try_join_all(resolution_tasks).await?;
}
```

**Resolution Algorithm**:

```rust
async fn resolve_schema_types(
    ctx: &CompileCtx,
    schema_id: &CacheKey,
    resolution_bar: &ProgressBar,
) -> Result<()> {
    let schema = ctx.get_dependency(&schema_id.package_name).await?;
    let resolution_levels = Self::namespace_levels(&schema).await;

    // Resolve namespaces depth-first
    for (depth, group) in resolution_levels.into_iter().enumerate() {
        let tasks: Vec<_> = group
            .iter()
            .map(|ns_name| Self::resolve_namespace_types(&schema, ns_name, &resolution_bar))
            .collect();

        futures_util::future::try_join_all(tasks).await?;
    }

    Ok(())
}
```

**Per-Namespace Resolution**:

```rust
async fn resolve_namespace_types(
    schema: &Arc<SchemaCtx>,
    ns_name: &str,
    resolution_bar: &ProgressBar,
) -> Result<()> {
    let ns = schema.get_namespace(ns_name)?;

    // Execute all 8 phases (see SPEC-0013)
    let resolver = TypeResolver::new(ns.clone());
    let resolution = resolver.resolve().await?;

    // Integrate results
    let mut ns_mut = ns.lock().await;
    ns_mut.integrate_resolution(resolution).await?;

    resolution_bar.inc(1);
    Ok(())
}
```

**Guarantee**: TypeResolver executes all 8 phases from SPEC-0013 for each namespace.

### Rule 10: Progress Reporting

**Requirement**: The compiler MUST provide real-time progress feedback for all compilation phases.

**Implementation** (from `parser/src/ctx/compile/progress.rs` and `schema_compiler.rs`):

**Dependency Loading**:

```rust
let resolving_spinner = progress.add_spinner("Resolving");
resolving_spinner.set_message(format!(
    "{}@{} ({} loaded)",
    result.package_name, result.version, loaded_count
));
resolving_spinner.finish_with_message(format!(
    "{} {}",
    loaded_count,
    if loaded_count != 1 { "dependencies" } else { "dependency" }
));
```

**Graph Building**:

```rust
let graph_spinner = ctx.progress.add_spinner("Analyzing");
graph_spinner.set_message("schema dependencies");
// ... build graph ...
graph_spinner.finish_with_message(format!("{} schemas", graph.nodes.len()));
```

**Schema Compilation**:

```rust
let total_schemas: u64 = groups.iter().map(|g| g.len() as u64).sum();
let compile_bar = ctx.progress.add_bar(total_schemas, "Compiling");

for (level, group) in groups.iter().enumerate() {
    // ... compile group ...
    compile_bar.inc(group.len() as u64);
    compile_bar.set_message(format!("level {} complete", level));
}

compile_bar.finish_with_message("all schemas");
```

**Type Resolution**:

```rust
let total_namespaces: u64 = /* count all namespaces */;
let resolution_bar = ctx.progress.add_bar(total_namespaces, "Resolving");

// ... resolve types ...
resolution_bar.inc(1);  // per namespace

resolution_bar.finish_with_message("all namespaces");
```

**Format**:

- **Spinners**: For indeterminate tasks (dependency loading, graph building)
- **Progress Bars**: For countable tasks (schema compilation, type resolution)
- **Messages**: Descriptive labels with counts

## Acceptance Criteria

- [x] SPEC-0014.AC-1: Dependency loading uses worker pool with `max_concurrent_tasks` limit
- [x] SPEC-0014.AC-2: Circular dependencies detected in `processing_set` during loading
- [x] SPEC-0014.AC-3: Dependency graph constructed from all schema imports
- [x] SPEC-0014.AC-4: Schema cycles detected using strongly connected components
- [x] SPEC-0014.AC-5: Schemas grouped into topological levels for compilation
- [x] SPEC-0014.AC-6: All schemas in topological group compile concurrently
- [x] SPEC-0014.AC-7: Namespaces ordered by depth using BFS within each schema
- [x] SPEC-0014.AC-8: Types registered before dependent namespaces compile
- [x] SPEC-0014.AC-9: Type resolution executes all 8 phases per namespace
- [x] SPEC-0014.AC-10: Progress indicators display compilation status in real-time

## References

- [RFC-0014](/specs/rfc/rfc-0014) — Parallel Compilation Design
- [AD-0002](/specs/ad/ad-0002) — Parallel Compilation Architecture
- [SPEC-0013](/specs/spec/spec-0013) — Type Resolution Phases
